library(rpart)
library(C50)
library(datasets)
ctrl <- trainControl(method = "repeatedcv",
repeats = 3,
verboseIter=TRUE)
j48mod <- train(Species ~ ., data = iris,
method = "J48",
tuneLength = 12,
trControl = ctrl)
summary(j48mod)
#tuneLength tells caret to try 12 different default settings
view (iris)
View(iris)
j48mod <- train(Species ~ ., data = iris,
method = "J48",
metric = "Accuracy",
tuneLength = 12,
trControl = ctrl)
j48mod <- train(Species ~ ., data = iris,
method = "J48",
metric = "Accuracy",
tuneLength = 12,
trControl = ctrl)
summary(j48mod$finalModel)
confusionMatrix.train(j48mod)
confusionMatrix.train(j48mod, norm="none")
confusionMatrix.train(j48mod, norm="overall")
confusionMatrix.train(j48mod, norm="average")
setwd("H:/MSc Data Science/CMM510- Data Mining/Week3 - 20191002")
library(caret)
library(mlbench)
library(RWeka)
library(rpart)
library(C50)
library(datasets)
ctrl <- trainControl(method = "repeatedcv",
repeats = 3,
verboseIter=TRUE)
j48mod <- train(Species ~ ., data = iris,
method = "J48",
tuneLength = 12,
trControl = ctrl)
summary(j48mod)
j48mod <- train(Species ~ ., data = iris,
method = "J48",
metric = "Accuracy",
tuneLength = 12,
trControl = ctrl)
summary(j48mod$finalModel)
confusionMatrix.train(j48mod)
j48mod <- train(Species ~ ., data = iris,
method = "J48",
metric = "Kappa",
tuneLength = 12,
trControl = ctrl)
summary(j48mod$finalModel)
confusionMatrix.train(j48mod)
confusionMatrix.train(j48mod, norm="none")
confusionMatrix.train(j48mod, norm="overall")
data(PimaIndiansDiabetes)
ctrl <- trainControl(method="repeatedcv", number=10, repeats=3)
Diabetes <- read.csv("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/diabetes.csv")
data(PimaIndiansDiabetes)
ctrl <- trainControl(method="repeatedcv", number=10, repeats=3)
View (Diabetes)
setwd("H:/MSc Data Science/CMM510- Data Mining/Week3 - 20191002")
library(caret)
library(mlbench)
library(RWeka)
library(rpart)
library(C50)
library(datasets)
#Diabetes <- read.csv("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/diabetes.csv")
data(PimaIndiansDiabetes)
ctrl <- trainControl(method="repeatedcv", number=10, repeats=3)
#use several algorithms: CART, kNN, J48.
# CART
# setting the seed makes the experiment reproducible.
set.seed(1)
mod.cart <- train(diabetes~.,
data=PimaIndiansDiabetes,
method="rpart",
trControl=ctrl)
# kNN
set.seed(1)
mod.knn <- train(diabetes~.,
data=PimaIndiansDiabetes,
method="knn",
trControl=ctrl)
# j48
set.seed(1)
mod.j48 <- train(diabetes~.,
data=PimaIndiansDiabetes,
method="J48",
trControl=ctrl)
# collect resamples
results <- resamples(list(CART=mod.cart, KNN=mod.knn, j48=mod.j48))
#Plotting results
scales <- list(x=list(relation="free"), y=list(relation= "free"))
dotplot(results, scales=scales, conf.level = 0.95)
pwd
cwd
wd
getwd
setwd("H:/MSc Data Science/CMM510- Data Mining/Week9 - 20191118")
#setwd("H:/MSc Data Science/CMM510- Data Mining/Week9 - 20191118")
library (mlbench)
library (caret)
library (caretEnsemble)
library (C50)
library (dplyr)
library (gbm)
#setwd("H:/MSc Data Science/CMM510- Data Mining/Week9 - 20191118")
library (mlbench)
library (caret)
library (caretEnsemble)
library (C50)
library (dplyr)
install (gbm)
#setwd("H:/MSc Data Science/CMM510- Data Mining/Week9 - 20191118")
library (mlbench)
library (caret)
library (caretEnsemble)
library (C50)
library (dplyr)
#install (gbm)
library (fastAdaboost)
library (xgboost)
library (ipred)
library (e1071)
library (randomForest)
library (pROC)
library (earth)
library (mda)
data(Ionosphere)
View(Ionosphere)
data(Ionosphere)
View(Ionosphere)
proc_ionos <- Ionosphere
proc_ionos <- proc_ionos[,-2]
proc_ionos$V1 <- as.numeric(as.character(proc_ionos$V1))
View(proc_ionos)
#setwd("H:/MSc Data Science/CMM510- Data Mining/Week9 - 20191118")
library (mlbench)
library (caret)
library (caretEnsemble)
library (C50)
library (dplyr)
install.packages ("gbm")
library (fastAdaboost)
library (xgboost)
library (ipred)
library (e1071)
library (randomForest)
library (pROC)
library (earth)
library (mda)
#setwd("H:/MSc Data Science/CMM510- Data Mining/Week9 - 20191118")
library (mlbench)
library (caret)
library (caretEnsemble)
library (C50)
library (dplyr)
library (gbm)
library (fastAdaboost)
library (xgboost)
library (ipred)
library (e1071)
library (randomForest)
library (pROC)
library (earth)
library (mda)
# J48
set.seed(1)
J48.mod <- train(Class~., data=proc_ionos, method="J48", metric="Accuracy", trControl=control)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# J48
set.seed(1)
J48.mod <- train(Class~., data=proc_ionos, method="J48", metric="Accuracy", trControl=control)
# C5.0
set.seed(1)
C50.mod <- train(Class~., data=proc_ionos, method="C5.0", metric="Accuracy", trControl=control)
# AdaBoost
set.seed(1)
adabo.mod <- train(Class~., data=proc_ionos, method="adaboost", metric="Accuracy", trControl=control, verbose=FALSE)
#Extreme gradient boosting
set.seed(1)
xgb.mod <- train(Class~., data=proc_ionos, method="xgbTree", metric="Accuracy", trControl=control, verbose=FALSE)
# Stochastic Gradient Boosting
set.seed(1)
gbm.mod <- train(Class~., data=proc_ionos, method="gbm", metric="Accuracy", trControl=control, verbose=FALSE)
results <- resamples(list(j48 = J48.mod, c5.0=C50.mod, adaboost= adabo.mod, xgb=xgb.mod, gbm=gbm.mod))
summary(results)
dotplot(results)
# plot 10 most important variables in the model.
plot(varImp(J48.mod), top=10)
# C5.0
set.seed(1)
C50.mod <- train(Class~., data=proc_ionos, method="C5.0", metric="Accuracy", trControl=control)
#Bagged Ada – this takes a while so you may skip it!
#set.seed(1)
#adab.mod <- train(Class~., data=proc_ionos, method="AdaBag", metric="Accuracy", trControl=control, verbose=FALSE)
# Bagged CART
set.seed(1)
bcart.mod <- train(Class~., data=proc_ionos, method="treebag", metric="Accuracy", trControl=control)
# Random Forest
set.seed(1)
rf.mod <- train(Class~., data=proc_ionos, method="rf", metric="Accuracy", trControl=control)
# summarize results – if you have skipped adabag, remove it from the results list
#results <- resamples(list(c50 = C50.mod, adabag = adab.mod, treebag=bcart.mod, rf=rf.mod))
results <- resamples(list(c50 = C50.mod, treebag=bcart.mod, rf=rf.mod))
summary(results)
dotplot(results)
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
baseAlgo <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
set.seed(1)
baseModels <- caretList(Class~., data=proc_ionos, trControl=control, methodList=baseAlgo)
results <- resamples(baseModels)
summary(results)
dotplot(results)
modelCor(results)
splom(results)
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(1)
ensembleStack <- caretStack(baseModels, method="glm", metric="Accuracy", trControl=stackControl)
print(ensembleStack)
contactLenses <- read.csv("contactLenses.csv", header=T)
contactLenses <- read.csv("H:\MSc Data Science\CMM510 - Data Mining\Week1 - 20190918\dataFiles\contactLenses.csv", header=T)
contactLenses <- read.csv("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/contactLenses.csv", header=T)
contactLenses <- read.csv("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/contactLenses.csv", header=T)
contactLenses <- read.csv("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/contactLenses.csv", header=T)
contactLenses <- read.csv ("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/contactLenses.csv", header=T)
contactLenses <- read.csv("H:/MSc Data Science/CMM510- Data Mining/Week1 - 20190918/dataFiles/contactLenses.csv", header=T)
contactLenses <- read.csv("H:/MSc Data Science/CMM510 - Data Mining/Week1 - 20190918/dataFiles/contactLenses.csv", header=T)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
library (naivebayes)
library (caret)
library(caret)
library (naivebayes)
library (caret)
library (naivebayes)
install.packages ("caret")
library (mlbench)
library (rWeka)
library (naivebayes)
install.packages ("caret")
library (mlbench)
install.packages ("rWeka")
library (naivebayes)
install.packages ("caret")
library (mlbench)
install.packages ("rWeka")
install.packages ("Rtools")
library (naivebayes)
install.packages ("caret")
library (mlbench)
install.packages ("RWeka")
install.packages ("Rtools")
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
library (naivebayes)
install.packages ("caret")
library (mlbench)
install.packages ("RWeka")
install.packages ("Rtools")
library (naivebayes)
library (caret)
library(caret)
library(RWeka)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
set.seed(1)
NBmodCL <- train(contactLenses ~., contactLenses,
method = "naive_bayes", na.action = na.omit,
trControl = ctrl)
library (naivebayes)
library (caret)
library(rlang)
install.packages("rlang")
library (naivebayes)
library (caret)
install.packages("rlang")
library (naivebayes)
library (caret)
library (naivebayes)
library (caret)
library (naivebayes)
library (caret)
library(caret)
library (naivebayes)
library (caret)
library (naivebayes)
library (caret)
library(caret)
library(rlang)
library (naivebayes)
library (caret)
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)
library (naivebayes)
library (caret)
?predict
library (naivebayes)
library (caret)
library(datasets)
WeatherPlay
library(partykit)
WeatherPlay
data("WeatherPlay")
rm WWeatherPlay
rm WeatherPlay
View(WeatherPlay)
drop WeatherPlay
??
#setwd("H:/MSc Data Science/CMM535 - Data Science Development/Week 1 - 20200128")
getwd()
setwd("H:/MSc Data Science/CMM535 - Data Science Development/Week 1 - 20200128")
getwd()
getwd()
# create some vectors
ints <- c(1,2,3,4,5)
ints1 <- c(1:10)
ints2 <- seq(1,10,by=2)
doubles <- seq(from=0,to=10,by=2.5)
chars <- character()
chars[1] <- "e" chars <- c(chars,"s")
# print vectors components
ints;ints1;ints2; chars;doubles
chars[1] <- "e" chars <- c(chars,"s")
chars[1] <- "e"
chars <- c(chars,"s")
rm contactLenses
View(contactLenses)
class(ints)# check the class of the vector ints
ints[length(ints)]# check the last element of the vector
class(chars)
class(doubles)
ints==ints1 # check if elements are equal
chars==ints # this will issue a warning, why?
x <- 1:10
y <- 1:10
x + y
x*10
# 4 x 4 matrix
myMat <- matrix (data=1:16,ncol = 4)
View (myMat)
myMat
myMat[1,2]# the element at row 1, column 2
myMat[2,4]# the element at row 2, column 4
# check matrix dimension
dim(myMat)
# sum the elements at the diagonal of the matrix
sum(diag(myMat))
mymat <- matrix(data=1:40, nrow=10)
mymat
mymat <- mymat[-2,]
mymat <- mymat[,-3]
print(mymat)
apply(myMat, 1, mean)
apply(mymat, 1, mean)
apply(myMat, 2, mean)
stIds <- c(1,2,3,4,5)
stNames<- c("St1","St2","St3","St4","St5")
stGrades<-c("Excellent","Good","Bad","Really Bad","Really Really Bad")
# create a data frame to combine the above vectors
df <- data.frame(student_id=stIds,student_name=stNames,student_grades=stGrades)
# print the data frame without row numbers
print(df,row.names = FALSE )
names(df) # pint out the column names of the data frame
head(df) # show the first few rows
# subsetting: is very important operation on data frame
# hide the student name
df[,c(1,3)]
# show records where students grades is "Bad"
df[df$studeng_grades=="Bad",]
# show records where students grades is "Bad"
df[df$student_grades=="Bad",]
# add a new column to the data frame
df$gpa <- NULL # generate some numbers between 40 and 90 and add it to the gpa column
df
df$gpa <- runif(1:nrow(df),40,90)
df
df$gpa <- runif(1:nrow(df),40,90)
df
df$gpa <- runif(1:nrow(df),80,90)
df
df$gpa <- runif(1:nrow(df),100,90)
df$gpa <- runif(1:nrow(df),40,90)
df
# print data frame contents
print(df,row.names = FALSE )
avgGPA <- mean(df$gpa)
lowestGPA <- min(df$gpa)
maxGPA <- max(df$gpa)
cat("Average GPA is: nt",avgGPA)
cat("Min GPA is: nt",lowestGPA)
cat("Max GPA is: nt",maxGPA)
cat("Average GPA is: \t",avgGPA)
cat("Min GPA is: \t",lowestGPA)
cat("Max GPA is: \t",maxGPA)
# first lets have a backup copy of our data frame
dfBackup <- df
# correct students grades
df$student_grades <- ifelse(df$gpa>=80,"Excellent",
ifelse(df$gpa>=70,"Very Good",
ifelse(df$gpa>=50,"Good",
ifelse(df$gpa>=40,"Bad","Very Bad"))))
# print the original and modified data frame
print(df,row.names = FALSE )
print(dfBackup,row.names=FALSE)
newStudent <- c(6,"ST6","Good",55.5)
df <- rbind(df,newStudent)
newStudent <- c(6,"ST6","Good",55.5)
df <- rbind(df,newStudent)
df
df [-7,]
df [6,2] < ST6
df [6,2] < "ST6"
class (df$student_name)
class (df$student_grades)
df$student_name <- as.character(df$student_name)
newStudent <- c(6,"ST6","Good",55.5)
df <- rbind(df,newStudent)
df
df [-6:8,]
df [-6,]
df [-(7:8),]
df [-6,]
df
df < df [-(6:8),]
df <- df [-(6:8),]
df
newStudent <- c(6,"ST6","Good",55.5)
df <- rbind(df,newStudent)
df
write.csv(dfBackup,"dfBackup.csv",row.names = FALSE)
write.csv(df,"df.csv",row.names = FALSE)
x <- runif(10,10,100)
mean(x)
median(x)
xRounded <- round(x,digits = 2)
x;xRounded
help("median")
# create a vector of doubles
myNumbers <- seq(from=-1,to=1,by=.1)
# function definition
squareVector <- function (x) f
return (x*x)
g
# call function
squaredX <- squareVector(myNumbers)
# function definition
squareVector <- function (x) {
return (x*x)
}
# call function
squaredX <- squareVector(myNumbers)
plot(squaredX,type='b',xlab = 'x', ylab = 'x*x', frame=FALSE,col='blue')
mynumbwa
myNumbers
x
# call function
squaredX
help("plot")
plot(squaredX,type='b',xlab = 'x', ylab = 'x*x', frame=FALSE,col='blue')
plot(squaredX,type='b',xlab = 'mynumbers', ylab = 'x*x', frame=FALSE,col='blue')
plot(myNumbers,squaredX,type='b',xlab = 'mynumbers', ylab = 'x*x', frame=FALSE,col='blue')
library(xtable)
\end{document}
data(mtcars)
data(mtcars)
data(mtcars)
summary(mtcars)
mtcars
mtcars$am == 1
cat(mtcars$am == 1)
mtcars[mtcars$am == 1,]
head (mtcars[mtcars$am == 1,],5)
@
head (mtcars[mtcars$am == 1,],5)
first5 <- head (mtcars[mtcars$am == 1,],5)
@
xtable (first5)
mean (mtcars$mpg)
manmpg <- mean (mtcars[mtcars$am==0]$mpg)
autompg <- mean (autocars$mpg)
autocars <- mtcars[mtcars$am == 1,]
autofirst5 <- head (autocars,5)
mean (mtcars$mpg)
autompg <- mean (autocars$mpg)
mancars <- mtcars[mtcars$am==0,]
manmpg <- mean (mancars$mpg)
data(mtcars)
summary(mtcars)
autompg <- mean (autocars$mpg)
mancars <- mtcars[mtcars$am==0,]
manmpg <- mean (mancars$mpg)
@
cat("Auto MPG: /t", autompg)
car("Man MPG: /t", manmpg)
manmpg <- mean (mancars$mpg)
data(mtcars)
summary(mtcars)
autompg <- mean (autocars$mpg)
mancars <- mtcars[mtcars$am==0,]
manmpg <- mean (mancars$mpg)
@
cat("Auto MPG: /t", autompg)
cat("Man MPG: /t", manmpg)
xtable(autofirst5)
getwd()
library(xtable)
xtable(mtcars)
